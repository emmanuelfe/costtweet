{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ce notebook classe des phases en 3 catégories \"cher\", \"moyen\",\"éco\".\n",
    "IL fait partie d'une série de notebook permettant de comparer différentes approches de classification en phasant varié les architectures de réseaux de neurones et les méthodes d'encodification des textes.\n",
    "\n",
    "Keras uses tensorflow uses sklearn uses numpy and pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Embedding, LSTM\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " DATA PROCESS\n",
    " les données ont été saisies sous excel et sauvegardées en csv utf-8 bizarement avec l'option séparateur virgules de microsoft\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDataset():\n",
    "\tdataset = pd.read_csv(\"RuleCout2.csv\", delimiter=\";\", encoding='utf-8')\n",
    "\t#print ( 'dataset loaded with shape',dataset.shape)\n",
    "\tX = dataset['text']\n",
    "\tY = dataset['class']\n",
    "\tdataset.set_index('text') #pour avoir les classes dans un ordre quelconque\n",
    "\treturn dataset, X, Y\n",
    "\n",
    "# utile pour les colonnes sur une ligne et afficher toutes les lignes\n",
    "pd.set_option('display.width', 200)\n",
    "pd.set_option('display.max_rows', 1000)\n",
    "\n",
    "# on ajoutera peu à peu de nouvelles colonnes au dataset \n",
    "dataset, X, Y = getDataset()\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Les mots les plus signifiants ont été classés par catégorie.\n",
    "\n",
    "La catégorie \"éco\" indique les mots qui vont dans le sens de choix économiques\n",
    "La catégorie \"troquet\" indique des classes de restaurant\n",
    "La catégorie \"négation\" indique des mots qui inverse le sens\n",
    "La catégorie \"éco\" indique les mots qui vont dans le sens de choix économiques\n",
    "La catégorie \"monnaie\" indique des quantités monayables\n",
    "La catégorie \"ambigu\" contient des termes qui se caractérisent par leur absence de positionnenement qui est une information en soit\n",
    "Les catégories \"nombres\" sont explicites.\n",
    "\n",
    "Les mots les plus signifiants vont permettre un encdage par catégorie.\n",
    "ansi \"pas trop cher\" deviendra \"négation 0 cher\" ou [5 0 10]. \n",
    "On se posera la question de la pérsence du zéro : est-elle bénéfique ou néfaste ?\n",
    "\n",
    "#### Quel est l'impact de ce travail de répartition sur  le taux de réussite ?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eco = ['modique','économique','ténu','japonais', 'chinois','kebab', 'self','pourri','avantageux','marché','bas',\n",
    "       'plein','petit','petite']\n",
    "troquet = ['troquet','bistrot','cantine','self','brasserie','restaurant','restau','resto']\n",
    "negation = ['pas','ni','peu']\n",
    "cher = ['cher','bon','haut','luxe','luxueux','fiche','fous','moyens', 'gastronomique', 'classe','riche','michelin','etoile','étoiles',\n",
    "'standing','haut','chicos','onéreux','onereux','super''meilleur','meilleurs','grattin','gratin', 'bien']\n",
    "monnaie = ['gamme', 'euro', 'prix', 'euros','problème','pb','qualité']\n",
    "ambigu = ['milieu','moyen']\n",
    "nombreEco = ['0','5','10','15','zéro','cinq','dix','quinze']\n",
    "nombreCher= ['20','30','40','50','80','100','cent','vingt','trente','quarante','cinquante','quatre-vingt','cent']\n",
    "limitatif = ['moins', 'peu','jusqu''à','plutôt']\n",
    "\n",
    "# le choix de l'entier n'importe pas sauf pour le zéro qui est la valeur complémentée par pad_sequences\n",
    "def score(word):\n",
    "\tif word in eco: \n",
    "\t\treturn .1\n",
    "\telif word in nombreEco:\n",
    "\t\treturn .2\n",
    "\telif word in troquet: \n",
    "\t\treturn .25\n",
    "\telif word in negation:\n",
    "\t\treturn .4\n",
    "\telif word in limitatif:\n",
    "\t\treturn .41\n",
    "\telif word in monnaie:\n",
    "\t\treturn .5\n",
    "\telif word in ambigu:\n",
    "\t\treturn .51\n",
    "\telif word in nombreCher:\n",
    "\t\treturn .8\n",
    "\telif word in cher:\n",
    "\t\treturn .9\n",
    "\telse: \n",
    "\t\treturn 0.01\n",
    "\t\n",
    "print ( 'score modique', score('modique'))\n",
    "print ( 'score troquet', score('troquet'))\n",
    "print ( 'score inconnu', score('inconnu'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ai préféré utiliser le tokenizer de keras plutôt que celui de nltk car les valeurs filtrées par défaut comprennent l'apostrophe\n",
    "def tokenize(sentence):\n",
    "\ttokens = text_to_word_sequence(sentence)\n",
    "\treturn tokens\n",
    "\t\n",
    "dataset['tokens']= dataset['text'].map(tokenize)\n",
    "dataset['tokens']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'étape suivante consiste à transformer une liste de tokens en une liste de nombres à encoder.\n",
    "Comme la taille de cette liste doit être fixe on utilise pad_sequence qui ajoute des zéros ou enlève des nombres. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token2code(tokens):\n",
    "\treturn list(map(score,tokens))\n",
    "\n",
    "#dataset['Xcodes'] = list (map(token2code,dataset['tokens']))\n",
    "def encode(data):\n",
    "\treturn dataset['tokens'].map(token2code)\n",
    "\n",
    "Xcodes = encode(dataset)\n",
    "XEncoded = pad_sequences(Xcodes,maxlen=5,padding='pre',dtype='float')\n",
    "dataset['XEncoded'] = XEncoded.tolist()\n",
    "dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le prochain pavé permet de transformer les catégories \"éco\",\"moyen\" et cher en vecteur.\n",
    "Ces vecteurs sont rangés dans la colonne 'Ycodes' du dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode class values as integers\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(Y)\n",
    "encoded_Y = encoder.transform(Y)\n",
    "# convert integers to dummy variables (i.e. one hot encoded)\n",
    "dummy_y = np_utils.to_categorical(encoded_Y)\n",
    "dataset['Ycodes'] = pd.Series(list(dummy_y))\n",
    "dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###MODEL DEFINITION\n",
    "Le réseau comprend trois couches.\n",
    "La première couche doit correspondre à la taille du vecteur X i.e au nombre de dimensions ici c'est cinq.\n",
    "La couche de sortie comprend 3 neurons avec une fonction d'activation softmax, cette répartition entre les trois poids indiquent une probabilité d'appartenir à une des trois classes.\n",
    "La taille de couche du milieu est ici de 11 avec une fonction d'activation Relu.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### MODEL DEFINITION\n",
    "#def sequential_model():\n",
    "#\t# create model\n",
    "#\tmodel = Sequential()\n",
    "#\tmodel.add(Dense(5, input_dim=5, activation='relu'))\n",
    "#\tmodel.add(Dense(3, activation='softmax'))\n",
    "#\t# Compile model\n",
    "#\tmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "#\treturn model\n",
    "max_features=5\n",
    "\n",
    "def sequential_model():\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(max_features, output_dim=5))\n",
    "    model.add(LSTM(32))\n",
    "    #model.add(Dropout(0.5))\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model = sequential_model()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### TRAINING + EVALUATION\n",
    "x_train = XEncoded\n",
    "y_train = dummy_y\n",
    "batch_size = 5\n",
    "model.fit(x_train, y_train,\n",
    "          epochs=10,\n",
    "          batch_size=batch_size)\n",
    "x_test = x_train\n",
    "y_test = y_train\n",
    "loss,metric = model.evaluate(x_test, y_test, batch_size=batch_size)\n",
    "print (\"Loss = \", loss,\" Accuracy = \",metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['predicted'] = list(model.predict(XEncoded)) \n",
    "\n",
    "def decodeY(L):\n",
    "\tif L[0]>L[1] and L[0]>L[2]: \n",
    "\t\treturn \"cher\"\n",
    "\telif L[2]>L[1] and L[2]>L[0]:\n",
    "\t\treturn \"éco\"\n",
    "\telse:\n",
    "\t\treturn \"moyen\"\n",
    "\t\n",
    "dataset['resultat']=list(map(decodeY,dataset['predicted']))\n",
    "dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
